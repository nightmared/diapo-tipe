\documentclass[ignorenonframetext,]{beamer}
\PassOptionsToPackage{hyphens}{url}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{textcomp} % provides euro and other symbols
\usetheme[]{theme}
%\IfFileExists{microtype.sty}{%
%\usepackage[]{microtype}
%\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
%}{}
\usepackage{tikz}
\usetikzlibrary{shadows,arrows,positioning}
\usepackage{pgfplots}
%\setlength{\parindent}{0pt}
%\setlength{\parskip}{6pt plus 2pt minus 1pt}
\urlstyle{same}  % don't use monospace font for urls
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Prevent slide breaks in the middle of a paragraph:
%\widowpenalties 1 10000
%\raggedbottom
\setbeamertemplate{section page}{
\begin{beamercolorbox}[sep=12pt,center]{part title}
  \usebeamerfont{section title}\insertsection\par
\end{beamercolorbox}
}
%\setlength{\emergencystretch}{3em}  % prevent overfull lines
%\setcounter{secnumdepth}{0}

\title{Reconnaissance d'une frontière à l'aide de réseaux neuronaux}
\author{Fabien Bernier, Elouen Ginat \& Simon Thoby}
\date{Juin 2018}

\begin{document}
\section{Les réseaux de neurones}
\frame{\titlepage}

\begin{frame}{Plan}
\tableofcontents
\end{frame}

\begin{frame}{Exemple d'image traitée}
	\center{\includegraphics[height=0.85\maxheight]{leletest.jpg}}
\end{frame}

\begin{frame}{Résultat attendu}
	\center{\includegraphics[height=0.85\maxheight]{leletest_courbe.jpg}}
\end{frame}

\begin{frame}{Structure générale}
	\includegraphics{net.jpg}
\end{frame}

%\begin{frame}{Fonctionnement du réseau}
%    - Accepte un vecteur en entrée : que l'on considére comme la dernière couche, notée $ Y^0 $ \\
%    - Pour chaque couche, on génére un nouveau vecteur à partir de la couche précédente en effectuant une somme pondérée des valeurs de la couche précédente : \[ Y^k = \tilde\alpha(W^k . Y^{k-1} + B^k) \]
%        Avec :
%    \begin{itemize}
%        \item $ \alpha $ : une fonction (dérivable) qui sert à améliorer l'efficacité du réseau en normalisant les sorties, classiquement $ \alpha : x \mapsto \frac{1}{1+e^{-x}} $
%        \item $\tilde\alpha$ : la fonction $\alpha$ appliquée à chacun des élements d'un vecteur.
%        \item $ W^n $ : les poids de passage de la couche $ n-1 $ à $ n $
%        \item $ B^n $ : les biais de la couche $ n $, qui permettent au réseau d'approximer des fonctions qui ne s'annulent pas quand l'entrée est nulle
%    \end{itemize}
%    - Retourne un vecteur en sortie : la sortie de la dernière couche, c'est-à-dire $ Y^n $
%\end{frame}

\begin{frame}{Fonctionnement du réseau}
	%On dispose pour créer le réseau de deux jeux de données assez conséquent (typiquement plusieurs milliers d'images dans notre cas) : l'un sert à entraîner le réseau et l'autre sert à en tester l'efficacité.
	\begin{center}
		$ Y^k = \tilde\alpha(W^k . Y^{k-1} + B^k) $ \\
	\end{center}
\begin{minipage}{0.5\textwidth}

    \begin{flalign*}
		& \tilde\alpha : X = \begin{pmatrix}
            x_1 \\
            x_2 \\
            \vdots \\
            x_l
        \end{pmatrix} \mapsto \begin{pmatrix}
            \alpha(x_1) \\
            \alpha(x_2) \\
            \vdots \\
            \alpha(x_l)
		\end{pmatrix} \\ 
				& W^k = \begin{pmatrix}
            w_{11}^k  & w_{21}^k & \cdots & w_{l1}^k \\
            w_{12}^k  & w_{22}^k & \cdots & w_{l2}^k \\
            \vdots & \vdots & \ddots & \vdots \\
            w_{1l}^k  & w_{2l}^k & \cdots & w_{ll}^k
		\end{pmatrix}
    \end{flalign*}
	\end{minipage}%
	\hfill%
	\begin{minipage}{0.5\textwidth}\raggedleft
		\begin{tikzpicture}
\begin{tiny}
    \begin{axis}[
    	legend pos=north west,
        axis x line=middle,
        axis y line=middle,
        %tick style={draw=none},
		%axis line style={draw=none},
        grid = major,
        width=6cm,
        height=4cm,
        grid style={dashed, gray!30},
        xmin=-5,     % start the diagram at this x-coordinate
        xmax= 5,    % end   the diagram at this x-coordinate
        ymin= -1,     % start the diagram at this y-coordinate
        ymax= 1,   % end   the diagram at this y-coordinate
        %axis background/.style={fill=white},
        %xlabel=x,
        %ylabel=y,
        %tick align=outside,
        enlargelimits=false]
      % plot the stirling-formulae
      \addplot[domain=-10:10, red, ultra thick,samples=100] {tanh(x)};
      \addlegendentry{$\alpha(x)=\tanh(x)$}
\end{axis}
\end{tiny}


	\end{tikzpicture}
		\[B^k = \begin{pmatrix}
            \alpha(b_1^k) \\
            \alpha(b_2^k) \\
            \vdots \\
            \alpha(b_l^k)
		\end{pmatrix} \]

	\end{minipage}

\end{frame}

\begin{frame}{Fonctionnement du programme}
	Deux réseaux de neurones distincts :\\
	\quad 1) Détection de la présence d'une frontière sur une image;\\
	\quad 2) Traitement des images contenant une frontière, et indication de la courbe de la frontière sur cette image.\\
	%On pourrait envisager que le réseau retourne des couples de positions pour la frontière, mais il n'est pas possible de connaître à l'avance la longueur de la frontière, et donc le nombre de sorties qui seraient nécessaires. Nous nous sommes donc tournés vers les courbes de Bézier. Il s'agit de courbes polynomiales définies par un certain nombres de points de contrôle (6 dans notre cas):\\
	\begin{center}
		\includegraphics[width=0.5\textwidth]{bezier.png}
	\end{center}
\end{frame}

\begin{frame}{Découpage d'une image}
	\center{\includegraphics[height=0.85\maxheight]{leletest_squared.jpg}}
\end{frame}

\begin{frame}{Structure du réseau}
	\noindent\begin{minipage}{0.82\textwidth}% adapt widths of minipages to your needs
	\textbf{Détection de la présence d'une frontière}\\
	\ Entrée \ $\longrightarrow$ \ Couche 1 $\longrightarrow$ Sortie \\
	\scriptsize{100 neurones $\longrightarrow$ 50 neurones $\longrightarrow$ 2 neurones}\\
\newline
\normalsize{\textbf{Détection des points de contrôles}}\\
	\ Entrée \ $\longrightarrow$ \ Couche 1 $\longrightarrow$ Couche 2 $\longrightarrow$ Couche 3 $\longrightarrow$ Sortie \\
	\scriptsize{400 neurones $\longrightarrow$ 200 neurones $\longrightarrow$ 100 neurones $\longrightarrow$ 50 neurones $\longrightarrow$ 8 neurones}
	\end{minipage}%
	\hfill%
	\begin{minipage}{0.18\textwidth}\raggedleft
	\includegraphics[width=0.7\linewidth]{sorties.png}
	\end{minipage}
\end{frame}

\section{La rétropropagation}
%\frame{\sectionpage}
%\begin{frame}{Approximation d'une fonction}
%	À l'instar du théorème d'approximation d'une fonction continue par un polynôme (Stone-Weierstrass), les réseaux de neurones ont pour but d'approximer une fonction dont on ne connaît généralement pas l'expression. Pour ce faire, on utilise généralement l'algorithme dit de «descente du gradient». Le principe est le suivant : \\
%    - Le gradient indique la direction de l'augmentation d'une fonction. \\
%    - On peut donc considérer la fonction $E$ qui indique l'«erreur» du réseau, c'est à dire la distance entre les sorties du réseau et les valeurs attendues en sortie. \\
%    - Notre but est donc de faire tendre les sorties du réseau vers les valeurs attendues, donc de minimiser cette fonction. \\
%\end{frame}

\begin{frame}{La rétropropagation}
    %Les données d'entrées ne sont pas des paramètres de notre fonction d'erreur, car on cherche à réduire l'erreur du réseau en modifiant les variables du réseau et non pas l'entrée, cela n'aurait pas de sens : $ E = E(w_{11}^1, w_{12}^n, ..., w_{ll}^n, b_1^1, n_2^1, ..., b_l^n) $.
    %Pour ce faire, on dérive l'erreur par rapport aux paramètres de notre réseau (les poids et les biais). \\
    %Puis on mets à jour les poids et les biais :
	\begin{align*}
		E &= \frac{1}{2} \sum_{i=1}^l (y_i^n-a_i)^2 \\
		&= E(w_{11}^1, w_{12}^1, ..., w_{ll}^n, b_1^1, b_2^1, ..., b_l^n) \\
		w_{ij}^k &\longleftarrow w_{ij}^k - \eta \frac{\partial{E}}{\partial{w_{ij}^k}} \\
		b_i^k &\longleftarrow b_i^k - \eta \frac{\partial{E}}{\partial{b_i^k}}
	\end{align*}
    %Les problèmes qui ont longtemps concentrés les efforts de la recherche sur les réseaux de neurones, outre la lourdeur de calcul, tournaient autour de la difficulté à concevoir et entraîner des réseaux multicouches. C'est là que l'algorithme de la rétropropagation appraît : il exploite le \textbf{théorème de dérivation des fonctions composées} pour calculer toutes ces dérivées.\\
    %L'algorithme est itératif, travaillant couche par couche, en partant de la dernière couche et en remontant jusqu'à la couche d'entrée du réseau, d'où son nom. \\
    %Un exemple assez classique de fonction d'erreur est la distance quadratique entre le vecteur obtenu et le vecteur attendu :
\end{frame}

\begin{frame}{La rétropropagation - Dernière couche}
    \begin{flalign*}
        \frac{\partial{E}}{\partial{b_j^n}} &= \frac{\partial{E}}{\partial{y_j^n}} \frac{\partial{y_j^n}}{\partial{b_j^n}} &\\
        \text{Or :}  &&\\
        \frac{\partial{E}}{\partial{y_j^n}} &= y_j^n-a_j &\\
        \frac{\partial{y_j^n}}{\partial{b_j^n}} &= \frac{\partial{(\alpha(\sum_{i=0}^l w_{ij}^n*y_i^{n-1} + b_j^n))}}{\partial{b_j^n}} = \alpha'(\sum_{i=0}^l w_{ij}^n*y_i^{n-1} + b_j^n) &\\
        \text{Donc :} &&\\
        \frac{\partial{E}}{\partial{b_j^n}} &= (y_j^n-a_j)*\alpha'(\sum_{i=0}^l w_{ij}^n*y_i^{n-1} + b_j^n) &
    \end{flalign*}
\end{frame}

\begin{frame}{La rétropropagation - Dernière couche}
    Il en va de même pour les poids:
    \begin{flalign*}
        \frac{\partial{E}}{\partial{w_{ij}^n}} &= \frac{\partial{E}}{\partial{y_j^n}} \frac{\partial{y_j^n}}{\partial{w_{ij}^n}} &\\
        \text{Or :}  &&&&\\
        \frac{\partial{E}}{\partial{y_j^n}} &= y_j^n-a_j &\\
        \frac{\partial{y_j^n}}{\partial{w_{ij}^n}} &= \frac{\partial{(\alpha(\sum_{i=0}^l w_{ij}^n*y_i^{n-1} + b_j^n))}}{\partial{w_{ij}^n}} = y_i^{n-1} * \alpha'(\sum_{i=0}^l w_{ij}^n*y_i^{n-1} + b_j^n) &\\
        \text{Donc :} &&&&\\
        \frac{\partial{E}}{\partial{w_{ij}^n}} &= y_i^{n-1}*(y_j^n-a_j)*\alpha'(\sum_{i=0}^l w_{ij}^n*y_i^{n-1} + b_j^n) &
    \end{flalign*}
\end{frame}
\begin{frame}{La rétropropagation - Couches cachées}
    %Les choses se compliquent néanmoins pour les couches précédentes: en effet, la dérivée de l'erreur en fonction du poids ou des biais d'un neurone de la couche $k$ dépend de la dérivée de l'erreur en fonction de tous les neurones de la couche $k+1$ :
	La dérivée de l'erreur en fonction du poids ou des biais d'un neurone de la couche $k$ dépend de la dérivée de l'erreur en fonction de tous les neurones de la couche $k+1$ :
    \[ \frac{\partial{E}}{\partial{b_j^k}} = \sum_{i=0}^l \frac{\partial{E}}{\partial{y_i^{k+1}}} \frac{\partial{y_i^{k+1}}}{\partial{b_j^k}} \]
%Fort heureusement, on vient de calculer $ \frac{\partial{E}}{\partial{y_i^n}} $ et on peut simplement calculer $ \frac{\partial{y_i^n}}{\partial{b_1^{n-1}}} $. On peut donc mettre à jour les poids et les biais de la couche $n-1$. En appliquant récursivement cette méthode, on peut ainsi actualiser tous les poids et tous les biais du réseau.
\end{frame}

%\section{Localisation des villes}
%\frame{\sectionpage}
%\begin{frame}{Base de données}
%    - 1615 images traitées manuellement (issues de la NASA) \\
%    - $\approx$ 5000 images générées artificiellement \\
%	\includegraphics[width=0.5\linewidth]{images-gen.png}
%	\includegraphics[width=0.5\linewidth]{images-real.png}
%\end{frame}
%\begin{frame}{Identifier la présence de villes - prétraitement}
%    Afin de ne passer aux réseau que des images qui comportent une ville, un prétraitement est appliqué: une convolution sur l'image puis un traitement par seuil est utilisé (si le pixel de couleur maximale obtenu excède un certain seuil, on considère que l'image comporte une ville et on demande alors au réseau d'identifier sa position).
%%\includegraphics{principe-convolution.png}
%%\includegraphics{image-exemple.png}
%%\includegraphics{convolution-exemple.png}
%Efficacité d'un algorithme à seuil:
%//TODO: better graph
%\includegraphics{filtre_vs_seuil.png}
%\end{frame}


\section{Fabrication d'une base de données}
%\frame{\sectionpage}

\begin{frame}{Milieux terre - mer}
	\includegraphics{fabricationImage.png}
\end{frame}

\begin{frame}{Frontière par une courbe de Bézier}
	\includegraphics{procede1.png}
\end{frame}

\begin{frame}{Application d'un flou et d'une rotation}
	\includegraphics{procede2.png}
	\begin{center}
		\includegraphics[width=.3\linewidth]{flou.png}
	\end{center}
\end{frame}

\begin{frame}{Variations lumineuses}
	\includegraphics{procede3.png}
\end{frame}

\begin{frame}{}
	\center{\includegraphics{tableau.png}}
\end{frame}

\section{Les résultats}
%\frame{\sectionpage}
\begin{frame}{Résultats sans prétraitement}
	\center{\includegraphics[height=0.8\maxheight]{border_bad_sob.jpeg}}
	\textit{\tiny{\\source : NASA}}
\end{frame}

\begin{frame}{Application de l'algorithme de Sobel}
	\includegraphics{sobel.png}
\end{frame}

\begin{frame}{Résultats avec le filtre de Sobel}
	\center{\includegraphics[height=0.8\maxheight]{border_good1.jpeg}}
	\textit{\tiny{\\source : NASA}}
\end{frame}
\begin{frame}{Résultats avec le filtre de Sobel}
	\center{\includegraphics[height=0.8\maxheight]{border_good.jpeg}}
	\textit{\tiny{\\source : IGN}}
\end{frame}
\begin{frame}{Résultats avec le filtre de Sobel}
	\center{\includegraphics[height=0.8\maxheight]{border_bad.jpeg}}
	\textit{\tiny{\\source : NASA}}
\end{frame}
%
%\begin{frame}{Résultats avec des subdivisions plus petites}
%	\center{\includegraphics[height=0.8\maxheight]{border_best.jpeg}}
%	\textit{\tiny{\\source : NASA}}
%\end{frame}

\begin{frame}{}
	\center{ \huge Merci de votre attention}
\end{frame}

\section{Annexe}
\begin{frame}{Un exemple simple: un porte logique ET à trois entrées}
	\begin{tikzpicture}
	[   cnode/.style={draw=black,fill=####1,minimum width=3mm,circle},
		]
		\node[cnode=red,label=0:$y_1^2$] (s) at (4.5,-2) {};
		\foreach \x in {1,...,3} {
			\node[cnode=blue,label=180:$y_\x^1$] (y1-\x) at (0,2-2*\x) {};
		}
		\foreach \x in {1,...,2} {
			\node[cnode=blue,label=90:$y_\x^2$] (y2-\x) at (2,1-2*\x) {};
			\draw (y2-\x) -- node[above,pos=0.4] {$w_{\x1}^2$} (s);
		}
		\foreach \x in {1,...,3} {
			\foreach \y in {1,...,2} {
				\draw (y1-\x) -- (y2-\y);
			}
		}
	\end{tikzpicture}
	\begin{tabular}{ c|c|c||c }
		A & B & C & Sortie \\
		\hline
		0 & 0 & 0 & 0 \\
		0 & 0 & 1 & 0 \\
		0 & 1 & 0 & 0 \\
		1 & 0 & 0 & 0 \\
		0 & 1 & 1 & 0 \\
		1 & 1 & 0 & 0 \\
		1 & 0 & 1 & 0 \\
		1 & 1 & 1 & 1 \\
	\end{tabular}
\end{frame}

\begin{frame}{Un exemple simple: un porte logique ET à trois entrées}
    $ \forall i \in \{1, 2\}, \forall k \in \{1, 2, 3\} $
    \begin{flalign*}
        y_1^1 &= w_{11}^1 * y_1^0 + w_{21}^1 * y_2^0 + w_{31}^1 * y_3^0 + b_1^1 &\\
        y_2^1 &= w_{12}^1 * y_1^0 + w_{22}^1 * y_2^0 + w_{32}^1 * y_3^0 + b_2^1 &\\
        y_1^2 &= w_{11}^2 * y_1^1 + w_{21}^2 * y_2^1 + b_1^2 &\\\\
        E &= \frac{(y_1^2-a_1)^2}{2} &\\\\
        \frac{\partial{E}}{\partial{b_1^2}} &= \frac{\partial{E}}{\partial{y_1^2}} = y_1^2-a_1 &\\
        \frac{\partial{E}}{\partial{w_{i1}^2}} &= y_i^1 * (y_1^2-a_1) &
    \end{flalign*}
\end{frame}

\begin{frame}{Un exemple simple: un porte logique ET à trois entrées}
    \begin{flalign*}
        \frac{\partial{E}}{\partial{b_i^1}} &= \frac{\partial{E}}{\partial{y_i^1}} &\\
        &= \frac{\partial{E}}{\partial{y_1^2}} \frac{\partial{y_1^2}}{\partial{y_1^1}} \frac{\partial{y_1^1}}{\partial{b_i^1}} &\\
        &= w_{i1}^2 * (y_1^2-a_1) &\\
        \frac{\partial{E}}{\partial{w_{ki}^1}} &= \frac{\partial{E}}{\partial{y_1^2}} \frac{\partial{y_1^2}}{\partial{y_1^1}} \frac{\partial{y_1^1}}{\partial{w_{ki}^1}} &\\
        &= y_k^0 * w_{i1}^2 * (y_1^2-a_1)
    \end{flalign*}
\end{frame}

\begin{frame}{Un exemple simple: un porte logique ET à trois entrées}
    \includegraphics{graph.png}
\end{frame}

\begin{frame}{L'algorithme de Sobel}
	$A = (a_{ij})$, matrice de l'image originale \\
	\hspace*{10pt}

	$ F_x = \left(
	\begin{array}{ccc}
	-1 & 0 & 1 \\ 
	-2 & 0 & 2 \\ 
	-1 & 0 & 1
	\end{array} \right) $
	\hspace*{60pt}
	$ F_y = \left(
	\begin{array}{ccc}
	-1 & -2 & -1 \\ 
	0 & 0 & 0 \\ 
	1 & 2 & 1
	\end{array} \right) $\\
	\vspace*{20pt}
	\[ c_{ij} = \sum^2_{p=0} \sum^2_{q=0} a_{(i+p),(j+q)} {f}_{pq} \]
	\begin{center}
		$g_{ij} = \sqrt{c_{x,ij}^2 + c_{y,ij}^2}$
	\end{center}
\end{frame}

\begin{frame}{L'algorithme de Sobel}
	\includegraphics{conv.jpeg}
\end{frame}


\end{document}
